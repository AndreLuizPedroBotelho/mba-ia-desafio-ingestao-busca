
# MBA IA - Desafio Ingest√£o e Busca

Projeto para ingest√£o de documentos PDF e busca inteligente via chat, utilizando embeddings generativos e banco vetorial PostgreSQL (PGVector).

Permite processar um PDF, indexar seu conte√∫do e realizar perguntas em linguagem natural sobre o documento.

## Sum√°rio
- [Requisitos](#requisitos)
- [Instala√ß√£o](#instala√ß√£o)
- [Uso](#uso)
- [Estrutura do Projeto](#estrutura-do-projeto)
- [Observa√ß√µes](#observa√ß√µes)
- [Licen√ßa](#licen√ßa)


## Requisitos
- Python 3.10+
- Conta Google Generative AI (API Key)
- PostgreSQL com extens√£o PGVector
- Docker


## Instala√ß√£o
1. Clone o reposit√≥rio:
   ```bash
   git clone <repo-url>
   cd mba-ia-desafio-ingestao-busca
   ```
2. (Opcional) Copie o arquivo de exemplo de vari√°veis de ambiente:
   ```bash
   cp .env.example .env
   ```
3. (Recomendado) Crie e ative um ambiente virtual (venv):
   - Linux/macOS:
     ```bash
     python3 -m venv .venv
     source .venv/bin/activate
     ```
   - Windows:
     ```cmd
     python -m venv .venv
     .venv\Scripts\activate
     ```
4. Instale as depend√™ncias:
   ```bash
   pip install -r requirements.txt
   ```
5. Edite o arquivo `.env` conforme necess√°rio:
   ```env
   LLM_API_KEY=                         # Chave da API Google Generative AI
   DATABASE_URL=                        # String de conex√£o PostgreSQL
   PG_VECTOR_COLLECTION_NAME=           # Nome da cole√ß√£o PGVector
   PDF_PATH=document.pdf                # Caminho do PDF a ser indexado
   LLM_MODEL=gemini-2.5-flash-lite      # (opcional) Modelo LLM usado na busca (default: gemini-2.5-flash-lite)
   LLM_MODEL_EMBEDDING=models/embedding-001 # (opcional) Modelo de embedding (default: models/embedding-001)
   ```
   - `LLM_API_KEY`: Chave da API Google Generative AI (obrigat√≥ria)
   - `DATABASE_URL`: String de conex√£o PostgreSQL+PGVector (obrigat√≥ria)
   - `PG_VECTOR_COLLECTION_NAME`: Nome da cole√ß√£o PGVector (obrigat√≥ria)
   - `PDF_PATH`: Caminho do PDF a ser indexado (obrigat√≥ria)
   - `LLM_MODEL`: Modelo LLM usado na busca (opcional, default: gemini-2.5-flash-lite)
   - `LLM_MODEL_EMBEDDING`: Modelo de embedding usado na indexa√ß√£o e busca vetorial (opcional, default: models/embedding-001)
   - O arquivo PDF deve estar no caminho especificado em `PDF_PATH`.
   - Se n√£o definir `LLM_MODEL` ou `LLM_MODEL_EMBEDDING`, ser√£o usados os valores padr√£o.

6. Suba um banco PostgreSQL+PGVector com Docker:
   ```bash
   docker-compose up -d
   ```
   Certifique-se de ajustar o `DATABASE_URL` conforme o servi√ßo do docker-compose.


## Uso
### 1. Ingest√£o do PDF
Executa a ingest√£o do PDF e armazena os embeddings no banco vetorial:
```bash
python src/ingest.py
```

### 2. Chat de Busca
Permite buscar informa√ß√µes no PDF via chat:
```bash
python src/chat.py
```
Digite sua pergunta e pressione Enter. Para sair, digite apenas `q`.

#### Exemplo de uso:
```
‚ùì Pergunta (q para sair): Qual o objetivo do documento?
üí¨ Resposta: ...
```


## Estrutura do Projeto
- `src/ingest.py`: Ingest√£o do PDF para o banco vetorial
- `src/chat.py`: Interface de chat para busca
- `src/search.py`: Fun√ß√µes auxiliares de busca

## Observa√ß√µes
- O projeto utiliza Google Generative AI Embeddings (√© necess√°rio chave de API v√°lida).
- O banco de dados deve estar acess√≠vel conforme a string `DATABASE_URL`.
- O chat s√≥ encerra se digitar exatamente `q` (min√∫sculo, sem espa√ßos).
- As vari√°veis `LLM_MODEL` e `LLM_MODEL_EMBEDDING` podem ser definidas no `.env` para customizar os modelos usados. Valores padr√£o: `gemini-2.5-flash-lite` e `models/embedding-001`.


## Licen√ßa
MIT